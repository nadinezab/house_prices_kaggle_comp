{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Acquire-the-data\" data-toc-modified-id=\"Acquire-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Acquire the data</a></span></li><li><span><a href=\"#Explore-the-data\" data-toc-modified-id=\"Explore-the-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Explore the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Investigate-target-variable\" data-toc-modified-id=\"Investigate-target-variable-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Investigate target variable</a></span></li><li><span><a href=\"#Investigate-Numeric-Features\" data-toc-modified-id=\"Investigate-Numeric-Features-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Investigate Numeric Features</a></span></li><li><span><a href=\"#Investigate-non-numeric-Features\" data-toc-modified-id=\"Investigate-non-numeric-Features-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Investigate non-numeric Features</a></span></li></ul></li><li><span><a href=\"#Transforming-and-engineering-features\" data-toc-modified-id=\"Transforming-and-engineering-features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Transforming and engineering features</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-hot-encoding-of-categorical-variables\" data-toc-modified-id=\"One-hot-encoding-of-categorical-variables-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>One-hot encoding of categorical variables</a></span></li><li><span><a href=\"#Deal-with-null-values\" data-toc-modified-id=\"Deal-with-null-values-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Deal with null values</a></span></li></ul></li><li><span><a href=\"#Build-Linear-Model\" data-toc-modified-id=\"Build-Linear-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build Linear Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluate-model\" data-toc-modified-id=\"Evaluate-model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Evaluate model</a></span></li></ul></li><li><span><a href=\"#Make-a-submission\" data-toc-modified-id=\"Make-a-submission-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Make a submission</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices: Advanced Regression Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll follow these steps to a successful Kaggle Competition submission:\n",
    "\n",
    "- Acquire the data\n",
    "- Explore the data\n",
    "- Engineer and transform the features and the target variable\n",
    "- Build a model\n",
    "- Make and submit predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first look at the `train.csv` data. After we’ve trained a model, we’ll make predictions using the `test.csv` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of data\n",
    "print (\"Train data shape:\", train.shape)\n",
    "print (\"Test data shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data does not include target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average sale price of a house in our dataset is close to USD 180 000 with most of the values falling within the USD 130,000 to USD 215,000 range.\n",
    "\n",
    "Next, we’ll check for skewness, which is a measure of the shape of the distribution of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"Skew is:\", train['SalePrice'].skew())\n",
    "plt.hist(train.SalePrice, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the distribution has a longer tail on the right. The distribution is positively skewed.\n",
    "\n",
    "Now we use `np.log()` to transform `SalePrice` and calculate the skewness a second time, as well as re-plot the data. A value closer to 0 means that we have improved the skewness of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log(train['SalePrice'])\n",
    "print (\"Skew is:\", target.skew())\n",
    "plt.hist(target, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see visually that the data better resembles a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numeric features\n",
    "numeric_features = train.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate non-numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(exclude=[np.number])\n",
    "categoricals.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count column indicates the count of non-null observations, while unique counts the number of unique values. top is the most commonly occurring value, with the frequency of the top value shown by freq.\n",
    "\n",
    "For many of these features, we might want to use one-hot encoding to make use of the information for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming and engineering features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When transforming features, it’s important to remember that any transformations that you’ve applied to the training data before fitting the model must be applied to the test data.\n",
    "\n",
    "Our model expects that the shape of the features from the train set match those from the test set. This means that any feature engineering that occurred while working on the train data should be applied again on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the `Street` data, which indicates whether there is `Gravel` or `Paved` road access to the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Street'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model needs numerical data, so we will use one-hot encoding to transform the data into a Boolean column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['enc_street'] = pd.get_dummies(train['Street'], drop_first=True)\n",
    "test['enc_street'] = pd.get_dummies(train['Street'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try engineering another feature. \n",
    "\n",
    "We’ll look at `SaleCondition` by constructing and plotting a pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_pivot = train.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)\n",
    "condition_pivot.plot(kind='bar', color='blue')\n",
    "plt.xlabel('Sale Condition')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `Partial` has a significantly higher Median Sale Price than the others. We will encode this as a new feature. We select all of the houses where `SaleCondition` is equal to `Patrial` and assign the value `1`, otherwise assign `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    return 1 if x == 'Partial' else 0\n",
    "\n",
    "train['enc_condition'] = train.SaleCondition.apply(encode)\n",
    "test['enc_condition'] = test.SaleCondition.apply(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of Null Values in each column\n",
    "train.isna().sum().sort_values(ascending = False)[:22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation can help us understand the missing values. In the case of PoolQC, the column refers to Pool Quality. Pool quality is NaN when PoolArea is 0, or there is no pool.\n",
    "We can find a similar relationship between many of the Garage-related columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll fill the missing values with an average value. This is a method of interpolation. The DataFrame.interpolate() method makes this simple.\n",
    "\n",
    "This is a quick and simple method of dealing with missing values, and might not lead to the best performance of the model on new data. Handling missing values is an important part of the modeling process, where creativity and insight can make a big difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.select_dtypes(include=[np.number]).interpolate().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "data.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(train['SalePrice'])\n",
    "X = data.drop(['Id', 'SalePrice'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear Regression model\n",
    "from sklearn import linear_model\n",
    "linreg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to fit the model using X_train and y_train. The `lr.fit()` method will fit the linear regression on the features and target variable that we pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle will evaluate our submission using root-mean-squared-error (RMSE). We’ll also look at The r-squared value. The r-squared value is a measure of how close the data are to the fitted regression line. It takes a value between 0 and 1, 1 meaning that all of the variance in the target is explained by the data. In general, a higher r-squared value means a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.score()` method returns the r-squared value by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"R^2 is: \\n\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our features explain approximately 86% of the variance in our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll consider RMSE. To do so, use the model we have built to make predictions on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.predict()` method will return a list of predictions given a set of predictors. Use model.predict() after fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_residuals = y_hat_test - y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mean_squared_error` function takes two arrays and calculates the rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE for test data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "\n",
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "actual_values = y_test\n",
    "plt.scatter(y_hat_test, actual_values, alpha=.7,\n",
    "            color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll need to create a `csv` that contains the predicted `SalePrice` for each observation in the `test.csv` dataset.\n",
    "\n",
    "The first column must the contain the ID from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Id'] = test['Id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, select the features from the test data for the model as we did above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features and drop ID, interpolate for missing values\n",
    "feats = test.select_dtypes(\n",
    "        include=[np.number]).drop(['Id'], axis=1).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = model.predict(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll transform the predictions to the correct form. Remember that to reverse `log()` we do `exp()`.\n",
    "So we will apply `np.exp()` to our predictions becasuse we have taken the logarithm previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxe exponential to reverse log\n",
    "final_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SalePrice in our submission DataFrame\n",
    "submission['SalePrice'] = final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we’re confident that we’ve got the data arranged in the proper format, we can export to a `.csv` file as Kaggle expects. We pass `index=False` because Pandas otherwise would create a new index for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve created a file called `submission1.csv` in our working directory that conforms to the correct format. Let's submit!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
